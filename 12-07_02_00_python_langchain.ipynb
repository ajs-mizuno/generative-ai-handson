{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6cc1613b",
            "metadata": {},
            "source": [
                "# LangChainの使い方\n",
                "このノートブックでは、最新の LangChain（v0.3~）を使った各用途別のサンプルを、逐次的に紹介します。"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "99349e2d",
            "metadata": {},
            "source": [
                "## 1. LLM ラッパー\n",
                "OpenAI の API 呼び出しを抽象化したモデルラッパーです。"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a135a9b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import AzureChatOpenAI\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# 生成AIのチャットモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# モデル実行\n",
                "print(llm.invoke([{\"role\": \"user\", \"content\": \"こんにちは\"}]).content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a122bf97",
            "metadata": {},
            "source": [
                "## 2. プロンプトテンプレートとチェイン"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a569e4be",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langchain_core.prompts.chat import PromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# 1.生成AIモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 2.プロンプト雛型を作成\n",
                "# input_variablesにパラメータを定義する\n",
                "# templateに雛型のプロンプトを作成し置換パラメータを埋め込む\n",
                "prompt = PromptTemplate(\n",
                "    input_variables=[\"language\", \"word\"],\n",
                "    template=\"次の単語を{language}語に翻訳してください：{word}\"\n",
                ")\n",
                "\n",
                "# 3.チェインを作成\n",
                "# 指示(Prompt)、モデル(Model)、回答のパーサー(Parser)を組み合わせたチェイン作成\n",
                "# StrOutputParserは回答を文字列として取得するパーサー\n",
                "chain = prompt | llm | StrOutputParser()\n",
                "\n",
                "# チェインを実行\n",
                "# 1つ目のチェインの指示に埋め込むパラメータを辞書形式で渡す\n",
                "print(chain.invoke({\n",
                "    \"language\": \"日本\",\n",
                "    \"word\": \"りんご\"\n",
                "}))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f172c7e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain.prompts import PromptTemplate\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# 1.生成AIモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 2.チェインの作成\n",
                "## 要約プロンプト\n",
                "summary_prompt = PromptTemplate(\n",
                "    input_variables=[\"text\"],\n",
                "    template=\"次の単語について解説してください。: {text}\"\n",
                ")\n",
                "## 要約チェイン\n",
                "summary_chain = summary_prompt | llm | StrOutputParser()\n",
                "\n",
                "## 翻訳プロンプト\n",
                "translate_prompt = PromptTemplate(\n",
                "    input_variables=[\"text\"],\n",
                "    template=\"英語に翻訳してください。: {text}\"\n",
                ")\n",
                "\n",
                "translate_chain = translate_prompt | llm | StrOutputParser()\n",
                "\n",
                "# 3.チェインを組み合わせ\n",
                "# smmaryの結果をtranslate_chainに渡す\n",
                "chain = summary_chain | translate_chain\n",
                "\n",
                "# チェインを実行\n",
                "# 単語を解説してから英語に翻訳\n",
                "print(chain.invoke(\"大規模言語モデル\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f7c6a612",
            "metadata": {},
            "source": [
                "チャット(やり取り)をテンプレート化"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b567ccb",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain.prompts import (\n",
                "    HumanMessagePromptTemplate,\n",
                "    SystemMessagePromptTemplate,\n",
                "    AIMessagePromptTemplate,\n",
                "    ChatPromptTemplate\n",
                ")\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# ------------------------------------------------------\n",
                "# 単一のチャットをテンプレート化する\n",
                "# ------------------------------------------------------\n",
                "\n",
                "# システムプロンプトをテンプレート化\n",
                "system_template = SystemMessagePromptTemplate.from_template(\n",
                "    \"これから提示するテーマに{language}で答えてください。\")\n",
                "print(system_template.format_messages(language=\"英語\"))\n",
                "# => [HumanMessage(content=\"これから提示するテーマに英語で答えてください。\")]\n",
                "\n",
                "# 人間からの単一メッセージをテンプレート化\n",
                "human_template = HumanMessagePromptTemplate.from_template(\"本日の{theme}について\")\n",
                "print(human_template.format_messages(theme=\"天気\"))\n",
                "# => [HumanMessage(content=\"User: 今日は天気は？\")]\n",
                "\n",
                "# AIの単一メッセージをテンプレート化\n",
                "ai_template = AIMessagePromptTemplate.from_template(\n",
                "    \"本日の{theme}は{answer}となっています。\")\n",
                "print(ai_template.format_messages(theme=\"天気\", answer=\"晴れ\"))\n",
                "# => [AIMessage(content='本日の天気は晴れとなっています。')]\n",
                "\n",
                "# ------------------------------------------------------\n",
                "# 単一のチャット雛型をまとめる。\n",
                "# ------------------------------------------------------\n",
                "print(\n",
                "    ChatPromptTemplate.from_messages([\n",
                "        system_template,\n",
                "        human_template,\n",
                "        ai_template\n",
                "    ]).format_prompt(\n",
                "        language=\"英語\",\n",
                "        theme=\"天気\",\n",
                "        answer=\"晴れ\"\n",
                "    ))\n",
                "\n",
                "# ------------------------------------------------------\n",
                "# 初めからまとめて作成する\n",
                "# ------------------------------------------------------\n",
                "# 複数のチャットをまとめて定義する\n",
                "chat_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"これから提示するテーマに{language}で答えてください。\"),\n",
                "    (\"human\", \"本日の{theme}について\"),\n",
                "    (\"ai\", \"本日の{theme}は{answer}となっています。\")\n",
                "])\n",
                "\n",
                "print(\n",
                "    chat_template.format_prompt(\n",
                "        language=\"英語\",\n",
                "        theme=\"天気\",\n",
                "        answer=\"晴れ\"\n",
                "    ))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7a2bcf14",
            "metadata": {},
            "source": [
                "チャットからテキスト生成を行う"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b13ebd6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain.prompts import ChatPromptTemplate\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# 1.生成AIモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 2.チャット雛型(あるいは履歴)を作成\n",
                "chat_template = ChatPromptTemplate.from_messages([\n",
                "    (\"system\", \"{theme}に関する話題について答えてください。\"),\n",
                "    (\"human\", \"{theme}の{question}は？\"),\n",
                "    (\"ai\", \"{theme}の{question}は{answer}となります。\"),\n",
                "    (\"human\", \"{additional}\")\n",
                "])\n",
                "\n",
                "# 3.チェインを組み立て\n",
                "chain = chat_template | llm | StrOutputParser()\n",
                "\n",
                "# チェインを実行\n",
                "chain.invoke({\n",
                "    \"theme\": \"本日\",\n",
                "    \"question\": \"天気\",\n",
                "    \"answer\": \"雨\",\n",
                "    \"additional\": \"傘は必要か？\"\n",
                "})"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f531f46c",
            "metadata": {},
            "source": [
                "## 4. エージェント（Agents:LangGraph版）\n",
                "LangChain版とLangGraph版があるが、LangGraph版への移行が推奨されているためLangGraph版で解説する"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5f39bdc",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langgraph.prebuilt import create_react_agent\n",
                "from langchain_core.tools import tool\n",
                "# from langchain.agents import load_tools\n",
                "from datetime import datetime,date\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "\n",
                "# 1.生成AIモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 2.ツールの設定\n",
                "# Note:LangChainには便利なツールが標準でいくつか定義されている。\n",
                "# ただし、ツールによっては外部サービスのAPIキーが必要になるので調べてみてください。\n",
                "# serpapiは、Google検索をプログラムで実行してくれるAPIサービスのツールですが、APIキーの取得が必要です。\n",
                "# tools = load_tools([\"serpapi\"], llm=llm)\n",
                "\n",
                "# 自作のツールを定義する\n",
                "# RAGを構築する際は検索する部分をフレームワークを使うか自作する\n",
                "\n",
                "@tool\n",
                "def today_is():\n",
                "    \"今日の日付を返す\"\n",
                "    return  datetime.today().date()\n",
                "\n",
                "@tool\n",
                "def fetch_weather(dt:date):\n",
                "    \"指定された日付の天気を返す\"\n",
                "    today = datetime.today().date()\n",
                "    if dt < today:\n",
                "        return \"雨\"\n",
                "    elif dt == today:\n",
                "        return \"曇りのち晴れ\"\n",
                "    else:\n",
                "        return \"晴れ\"\n",
                "\n",
                "tools = [\n",
                "    today_is,\n",
                "    fetch_weather\n",
                "]\n",
                "\n",
                "# エージェントを構築する\n",
                "agent = create_react_agent(model=llm, tools=tools, prompt=\"You are a helpful assistant\")\n",
                "agent.invoke({\n",
                "    \"messages\": [{\"role\": \"user\", \"content\": \"今日の日付と天気は？\"}]\n",
                "})\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d32e144f",
            "metadata": {},
            "source": [
                "## 5. メモリ管理（LangGraph 永続化）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fce83fb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from typing import Annotated, TypedDict, Union\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "from langgraph.graph import StateGraph\n",
                "from langgraph.graph.message import add_messages\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# 環境設定を読込\n",
                "load_dotenv()\n",
                "\n",
                "# 1. 会話履歴を State に定義\n",
                "class State(TypedDict):\n",
                "    messages: Annotated[list[Union[HumanMessage, AIMessage]], add_messages]\n",
                "\n",
                "# 2.生成AIモデルを作成\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 3. ノード定義：履歴を渡して応答を得る\n",
                "def chatbot_node(state: State) -> dict:\n",
                "    # 最新のメッセージを含む履歴をモデルに渡す\n",
                "    response: AIMessage = llm.invoke(state[\"messages\"])\n",
                "    # 応答を履歴に追加して返す\n",
                "    return {\"messages\": state[\"messages\"] + [response]}\n",
                "# 4. グラフのビルド＆コンパイル（MemorySaver を渡す）\n",
                "builder = StateGraph(State)\n",
                "builder.add_node(\"chatbot\", chatbot_node)\n",
                "builder.set_entry_point(\"chatbot\")\n",
                "memory = MemorySaver()  # in-memory checkpointer\n",
                "graph = builder.compile(checkpointer=memory)\n",
                "\n",
                "# 5. 同じ thread_id で実行すると前回までの履歴を保持\n",
                "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
                "\n",
                "# 最初の発話\n",
                "initial = [HumanMessage(content=\"こんにちは、今日の日付は2025年の7月10日です。あなたは誰ですか？\")]\n",
                "result = graph.invoke({\"messages\": initial}, config)\n",
                "print(result[\"messages\"][-1].content)  # => AI の応答\n",
                "\n",
                "# フォローアップ\n",
                "followup = [HumanMessage(content=\"今日の日付はいつでしたっけ？\")]\n",
                "result = graph.invoke({\"messages\": followup}, config)\n",
                "print(result[\"messages\"][-1].content)  # => 文脈を踏まえた応答"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2cae75b",
            "metadata": {},
            "source": [
                "# LangChainの拡張機能を利用した処理\n",
                "LangChainに関連する機能をサードパーティが作成したものをまとめたもの(Langchain_community)を利用した実装例"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb23384e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# コミュニティの追加ライブラリを導入\n",
                "%pip install langchain_community pypdf"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ab178830",
            "metadata": {},
            "source": [
                "\n",
                "## ドキュメントロード & テキスト分割"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0e91653a",
            "metadata": {},
            "source": [
                "### 1000文字ごとに分割するが、前後文章の200文字は重複して取得する"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fb852699",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "loader = PyPDFLoader(\"sample.pdf\")\n",
                "docs = loader.load()\n",
                "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
                "chunks = splitter.split_documents(docs)\n",
                "print(f\"チャンク数: {len(chunks)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d260d39",
            "metadata": {},
            "source": [
                "## 埋め込み & ベクターストア\n",
                "[python_weaviate](12-04_00_00_python_weaviate.ipynb)のサンプルコードを参照"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e65d8336",
            "metadata": {},
            "source": [
                "## 検索＋回答（Retrieval Chain）"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2359457",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import weaviate\n",
                "from dotenv import load_dotenv\n",
                "from langchain.chains import create_retrieval_chain\n",
                "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
                "from langchain_core.prompts import PromptTemplate  # または langchain.prompts.prompt\n",
                "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
                "from langchain_weaviate import WeaviateVectorStore\n",
                "\n",
                "# 1. 接続情報の読込\n",
                "load_dotenv()\n",
                "\n",
                "# 2. Weaviate クライアント初期化 (ローカル)\n",
                "client = weaviate.connect_to_local()\n",
                "\n",
                "# 3. Embeddings モデルの初期化\n",
                "embeddings = AzureOpenAIEmbeddings(\n",
                "    deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_API_VERSION\")\n",
                ")\n",
                "\n",
                "# 4. WeaviateVectorStore のセットアップ\n",
                "vectorstore = WeaviateVectorStore(\n",
                "    client=client,\n",
                "    index_name=\"Document\",       # Weaviate 側のクラス名\n",
                "    text_key=\"content\",          # プロパティ名 (本文格納フィールド)\n",
                "    embedding=embeddings,\n",
                ")\n",
                "\n",
                "# 5. LLMの初期化\n",
                "llm = AzureChatOpenAI(\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 6. プロンプトを作成\n",
                "prompt = PromptTemplate(\n",
                "    input_variables=[\"context\", \"input\"],\n",
                "    template=\"\"\"\n",
                "次の文脈をもとに質問に答えてください:\n",
                "{context}\n",
                "ユーザ: {input}\n",
                "アシスタント:\"\"\")\n",
                "\n",
                "# 7. 回答のチェインを定義\n",
                "combine_chain = create_stuff_documents_chain(\n",
                "    llm=llm,\n",
                "    prompt=prompt\n",
                ")\n",
                "\n",
                "# 8.回答のチェインを初期化\n",
                "qa_chain = create_retrieval_chain(\n",
                "    retriever=vectorstore.as_retriever(),\n",
                "    combine_docs_chain=combine_chain,\n",
                ")\n",
                "\n",
                "# 9. チェインを実行\n",
                "result = qa_chain.invoke({\"input\": \"LangChain のメモリ管理方法は？\"})\n",
                "print(result[\"answer\"])\n",
                "\n",
                "# 10. クライアントのクローズ\n",
                "client.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "35c6d2a7",
            "metadata": {},
            "source": [
                "## コールバック（ストリーミング出力）\n",
                "生成AIが生成する回答がすべて得られるまで待たず、帰ってきた文字を都度、処理する方式で実装する場合の書き方"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f17af1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "from langchain_openai import AzureChatOpenAI\n",
                "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
                "\n",
                "# 動作確認用（ストリームで受信したデータを標準出力に表示する）\n",
                "from langchain.callbacks.manager import CallbackManager\n",
                "\n",
                "# 1. 接続情報の読込\n",
                "load_dotenv()\n",
                "\n",
                "# 2. ストリームハンドラを作成\n",
                "# ストリーム受信時のコールバック関数をハンドラに設定\n",
                "streaming_handler = StreamingStdOutCallbackHandler()\n",
                "\n",
                "# 3. ストリーミング設定でLLMの初期化\n",
                "llm_stream = AzureChatOpenAI(\n",
                "    streaming=True,\n",
                "    callbacks=[streaming_handler],\n",
                "    azure_deployment=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_MODEL\"),\n",
                "    api_version=os.environ.get(\"AZURE_OPENAI_LLM_DEPLOYMENT_API_VERSION\"),\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# 4.実行\n",
                "llm_stream.invoke([{\"role\":\"user\",\"content\":\"最新ニュースを教えて\"}])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
