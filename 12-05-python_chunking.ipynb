{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f41fc",
   "metadata": {},
   "source": [
    "# 📄 テキストチャンク化教材：RAG のための事前処理と設計\n",
    "\n",
    "この教材では、RAG（Retrieval-Augmented Generation）構成の事前処理として不可欠な「チャンク化（text chunking）」について、背景知識から Python での実装例までを段階的に学びます。Weaviate や LangChain などと組み合わせる前提の教材です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c83319",
   "metadata": {},
   "source": [
    "## ✅ ステップ ①：チャンク化とは？\n",
    "\n",
    "### 📌 概要\n",
    "\n",
    "- チャンク（chunk）＝検索・埋め込み用の**適切な長さの分割テキスト**\n",
    "- 多くの LLM はトークン数制限があるため、**長文は分割して扱う必要**がある\n",
    "- 例：PDF・HTML・Markdown・議事録などの文書を 300〜500 文字ごとに分割\n",
    "\n",
    "### 📌 なぜ重要か？\n",
    "\n",
    "- 細かすぎる → コンテキストが失われる\n",
    "- 長すぎる → 入力制限・ノイズが増える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce507184",
   "metadata": {},
   "source": [
    "## ✅ ステップ ②：分割の方法と戦略\n",
    "\n",
    "### 📌 固定文字数で分割（シンプル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb68e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...長いテキスト...']\n"
     ]
    }
   ],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "text = \"...長いテキスト...\"\n",
    "chunks = wrap(text, width=300)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2651d",
   "metadata": {},
   "source": [
    "### 📌 センテンス単位 + トークン考慮（NLTK など）\n",
    "\n",
    "#### 英語など欧文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \" \"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \" \"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706bcfc",
   "metadata": {},
   "source": [
    "#### 日本語（NLTK は不向き、代替として`janome`や`fugashi`などを使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text = \"これは日本語の文です。文を分割して処理します。\"\n",
    "t = Tokenizer()\n",
    "sentences = text.split(\"。\")\n",
    "\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        continue\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \"。\"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \"。\"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e9b59",
   "metadata": {},
   "source": [
    "### 📌 LangChain の`RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"、\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027eb2fb",
   "metadata": {},
   "source": [
    "## ✅ ステップ ③：正規表現を用いた前処理・整形\n",
    "\n",
    "### 📌 ノイズ除去や整形の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa65001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 改行・空白の正規化\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "# 特定の記号を除去\n",
    "text = re.sub(r\"[【】『』◇◆■□●○▲▼☆★▶︎→→]\", \"\", text)\n",
    "\n",
    "# セクション見出しで分割（例：\"第1章\"）\n",
    "chunks = re.split(r\"第\\d+章\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa109d",
   "metadata": {},
   "source": [
    "## ✅ ステップ ④：ファイルごとのチャンク化\n",
    "\n",
    "### 📌 PDF の場合（PyMuPDF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open(\"document.pdf\")\n",
    "text = \"\\n\".join([page.get_text() for page in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1efa",
   "metadata": {},
   "source": [
    "### 📌 Markdown/HTML/Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.md\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2481ee",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑤：チャンクにメタデータを付加\n",
    "\n",
    "### 📌 ファイル名・ページ番号などを保持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_records = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_records.append({\n",
    "        \"content\": chunk,\n",
    "        \"source\": \"document.pdf\",\n",
    "        \"page\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8dd50",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑥：Weaviate への登録\n",
    "\n",
    "### 📌 チャンクとメタデータを Weaviate に保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07725124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# スキーマが未作成の場合は事前に作成する\n",
    "class_obj = {\n",
    "    \"class\": \"DocumentChunk\",\n",
    "    \"properties\": [\n",
    "        {\"name\": \"content\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"source\", \"dataType\": [\"text\"]},\n",
    "        {\"name\": \"page\", \"dataType\": [\"int\"]}\n",
    "    ],\n",
    "    \"vectorizer\": \"text2vec-openai\"\n",
    "}\n",
    "client.schema.create_class(class_obj)\n",
    "\n",
    "# チャンクを登録\n",
    "for record in chunk_records:\n",
    "    client.data_object.create(record, class_name=\"DocumentChunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d7b6d",
   "metadata": {},
   "source": [
    "## ✅ 実践課題\n",
    "\n",
    "### 📁 課題 1：任意のテキストファイルを 300 文字ごとに分割し、チャンク一覧を出力\n",
    "\n",
    "### 📁 課題 2：Markdown 文書から文単位チャンクを抽出し、章ごとのチャンク一覧を作成\n",
    "\n",
    "### 📁 課題 3：チャンクごとにメタデータ（セクション名・出典など）を保持し、CSV または JSON に出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be5dbb",
   "metadata": {},
   "source": [
    "### 🚀 最終ゴール\n",
    "\n",
    "- LLM やベクトル DB と組み合わせる前提として、**扱いやすく検索に適したテキストチャンクを自動生成**できるようになること。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
