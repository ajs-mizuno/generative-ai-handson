{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f41fc",
   "metadata": {},
   "source": [
    "# 📄 テキストチャンク化教材：RAG のための事前処理と設計\n",
    "\n",
    "この教材では、RAG（Retrieval-Augmented Generation）構成の事前処理として不可欠な「チャンク化（text chunking）」について、背景知識から Python での実装例までを段階的に学びます。Weaviate や LangChain などと組み合わせる前提の教材です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c83319",
   "metadata": {},
   "source": [
    "## ✅ ステップ ①：チャンク化とは？\n",
    "\n",
    "### 📌 概要\n",
    "\n",
    "- チャンク（chunk）＝検索・埋め込み用の**適切な長さの分割テキスト**\n",
    "- 多くの LLM はトークン数制限があるため、**長文は分割して扱う必要**がある\n",
    "- 例：PDF・HTML・Markdown・議事録などの文書を 300〜500 文字ごとに分割\n",
    "\n",
    "### 📌 なぜ重要か？\n",
    "\n",
    "- 細かすぎる → コンテキストが失われる\n",
    "- 長すぎる → 入力制限・ノイズが増える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce507184",
   "metadata": {},
   "source": [
    "## ✅ ステップ ②：分割の方法と戦略\n",
    "\n",
    "### 📌 固定文字数で分割（シンプル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "text = \"...長いテキスト...\"\n",
    "chunks = wrap(text, width=300)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2651d",
   "metadata": {},
   "source": [
    "### 📌 センテンス単位 + トークン考慮（NLTK など）\n",
    "\n",
    "#### 英語など欧文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d052",
   "metadata": {},
   "source": [
    "ライブラリで利用するNLTKデータを別途ダウンロードする  \n",
    "辞書みたいなものなので結構容量があると思われるためセンテンス単位の分析をしたい人は各自で実施ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c47d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \" \"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \" \"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706bcfc",
   "metadata": {},
   "source": [
    "#### 日本語（NLTK は不向き、代替として`janome`や`fugashi`などを使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text = \"これは日本語の文です。文を分割して処理します。\"\n",
    "t = Tokenizer()\n",
    "sentences = text.split(\"。\")\n",
    "\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        continue\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \"。\"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \"。\"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e9b59",
   "metadata": {},
   "source": [
    "### 📌 LangChain の`RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Wikiより引用\n",
    "# https://ja.wikipedia.org/wiki/%E7%94%9F%E6%88%90%E7%9A%84%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD\n",
    "text = \"生成的人工知能（せいせいてきじんこうちのう、英: generative artificial intelligence）または生成AI（せいせいエーアイ、英: GenAI）は、文字などの入力（プロンプト）に対してテキスト、画像、または他のメディアを応答として生成する人工知能システムの一種である[5][6]。ジェネレーティブAI、ジェネラティブAIともよばれる。生成的人工知能モデルは、訓練データの規則性や構造を訓練において学習することで、訓練データに含まれない新しいデータを生成することができる[7][8]。著名な生成AIシステムとして、OpenAIがGPT-3やGPT-4の大規模言語モデル[9]を使用して構築したチャットボットのChatGPT（および別形のBing Chat）や、GoogleがLaMDA基盤モデルに構築したチャットボットBardがある[10]。その他の生成AIモデルとして、Stable DiffusionやDALL-Eなどの人工知能アートシステムがあげられる[11]。生成AIは、アート、執筆、ソフトウェア開発、ヘルスケア、金融、ゲーム、マーケティング、ファッションなど、幅広い業界で応用できる可能性があるとされている[12][13]。生成AIへの投資は2020年代初頭に急増し、Microsoft、Google、Baiduなどの大企業だけでなく、多数の中小企業も生成AIモデルを開発している[5][14][15]。しかし、生成AIを訓練する目的での著作物の野放図な利用や人をだましたり操作したりするフェイクニュースやディープフェイクの作成など、生成AIの悪用の可能性も懸念されており[16][17][18]、欧州連合における人工知能法など法規制の議論も進んでいる[19][20]。また、効果的加速主義などの技術思想との関係も指摘されている[21]。\"\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"、\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_text(text)\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027eb2fb",
   "metadata": {},
   "source": [
    "## ✅ ステップ ③：正規表現を用いた前処理・整形\n",
    "\n",
    "### 📌 ノイズ除去や整形の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa65001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 改行・空白の正規化\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "# 特定の記号を除去\n",
    "text = re.sub(r\"[【】『』◇◆■□●○▲▼☆★▶︎→→]\", \"\", text)\n",
    "\n",
    "# セクション見出しで分割（例：\"第1章\"）\n",
    "chunks = re.split(r\"第\\d+章\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa109d",
   "metadata": {},
   "source": [
    "## ✅ ステップ ④：ファイルごとのチャンク化\n",
    "\n",
    "### 📌 PDF の場合（PyMuPDF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open(\"document.pdf\")\n",
    "text = \"\\n\".join([page.get_text() for page in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1efa",
   "metadata": {},
   "source": [
    "### 📌 Markdown/HTML/Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.md\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2481ee",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑤：チャンクにメタデータを付加\n",
    "\n",
    "### 📌 ファイル名・ページ番号などを保持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_records = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_records.append({\n",
    "        \"content\": chunk,\n",
    "        \"source\": \"document.pdf\",\n",
    "        \"page\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8dd50",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑥：ベクトルデータベース(Weaviate) への登録\n",
    "\n",
    "### 📌 1.コレクションを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07725124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wvcc\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Weaviateにローカル接続\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# スキーマが未作成の場合は事前に作成する\n",
    "if not client.collections.exists(\"DocumentChunk\"):\n",
    "    collection = client.collections.create(\n",
    "        name=\"DocumentChunk\",\n",
    "        description=\"チャンクしたデータを保存する演習用コレクション\",\n",
    "        properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"source\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"page\", data_type=wvcc.DataType.INT)\n",
    "        ],\n",
    "        vectorizer_config=wvcc.Configure.Vectorizer.text2vec_azure_openai(\n",
    "            base_url=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            deployment_id=os.environ.get(\n",
    "                \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_MODEL\"),\n",
    "            resource_name=os.environ.get(\"AZURE_OPENAI_RESOURCE_NAME\")\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(f\"すでにコレクションが存在します: DocumentChunk\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f82b20",
   "metadata": {},
   "source": [
    "### 📌 2.チャンクを登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de44a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Weaviateにローカル接続\n",
    "client = weaviate.connect_to_local()\n",
    "doc_collection = client.collections.get(\"DocumentChunk\")\n",
    "\n",
    "# チャンクを登録\n",
    "for i, record in enumerate(chunk_records):\n",
    "    # サンプルドキュメントを登録\n",
    "    # recordにはDocumentChunkで定義された書式のデータ\n",
    "    # 例) {\"content\": \"XXXX\", \"source\":\"YYYY\", \"page\": 1}\n",
    "    uuid = doc_collection.data.insert(properties=record)\n",
    "    print(f\"Inserted object no:{i}、UUID: {uuid}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d7b6d",
   "metadata": {},
   "source": [
    "## ✅ 実践課題\n",
    "\n",
    "### 📁 課題 1：任意のテキストファイルを 300 文字ごとに分割し、チャンク一覧を出力\n",
    "\n",
    "### 📁 課題 2：Markdown 文書から文単位チャンクを抽出し、章ごとのチャンク一覧を作成\n",
    "\n",
    "### 📁 課題 3：チャンクごとにメタデータ（セクション名・出典など）を保持し、CSV または JSON に出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be5dbb",
   "metadata": {},
   "source": [
    "### 🚀 最終ゴール\n",
    "\n",
    "- LLM やベクトル DB と組み合わせる前提として、**扱いやすく検索に適したテキストチャンクを自動生成**できるようになること。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
