{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4f41fc",
   "metadata": {},
   "source": [
    "# 📄 テキストチャンク化教材：RAG のための事前処理と設計\n",
    "\n",
    "この教材では、RAG（Retrieval-Augmented Generation）構成の事前処理として不可欠な「チャンク化（text chunking）」について、背景知識から Python での実装例までを段階的に学びます。Weaviate や LangChain などと組み合わせる前提の教材です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c83319",
   "metadata": {},
   "source": [
    "## ✅ ステップ ①：チャンク化とは？\n",
    "\n",
    "### 📌 概要\n",
    "\n",
    "- チャンク（chunk）＝検索・埋め込み用の**適切な長さの分割テキスト**\n",
    "- 多くの LLM はトークン数制限があるため、**長文は分割して扱う必要**がある\n",
    "- 例：PDF・HTML・Markdown・議事録などの文書を 300〜500 文字ごとに分割\n",
    "\n",
    "### 📌 なぜ重要か？\n",
    "\n",
    "- 細かすぎる → コンテキストが失われる\n",
    "- 長すぎる → 入力制限・ノイズが増える"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce507184",
   "metadata": {},
   "source": [
    "## ✅ ステップ ②：分割の方法と戦略\n",
    "\n",
    "### 📌 固定文字数で分割（シンプル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "text = \"...長いテキスト...\"\n",
    "chunks = wrap(text, width=300)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2651d",
   "metadata": {},
   "source": [
    "### 📌 センテンス単位 + トークン考慮（NLTK など）\n",
    "\n",
    "#### 英語など欧文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d052",
   "metadata": {},
   "source": [
    "ライブラリで利用するNLTKデータを別途ダウンロードする  \n",
    "辞書みたいなものなので結構容量があると思われるためセンテンス単位の分析をしたい人は各自で実施ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c47d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \" \"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \" \"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706bcfc",
   "metadata": {},
   "source": [
    "#### 日本語（NLTK は不向き、代替として`janome`や`fugashi`などを使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text = \"これは日本語の文です。文を分割して処理します。\"\n",
    "t = Tokenizer()\n",
    "sentences = text.split(\"。\")\n",
    "\n",
    "chunks = []\n",
    "buffer = \"\"\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    if not sentence:\n",
    "        continue\n",
    "    if len(buffer + sentence) < 400:\n",
    "        buffer += sentence + \"。\"\n",
    "    else:\n",
    "        chunks.append(buffer.strip())\n",
    "        buffer = sentence + \"。\"\n",
    "if buffer:\n",
    "    chunks.append(buffer.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e9b59",
   "metadata": {},
   "source": [
    "### 📌 LangChain の`RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df7574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"、\", \" \"]\n",
    ")\n",
    "chunks = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027eb2fb",
   "metadata": {},
   "source": [
    "## ✅ ステップ ③：正規表現を用いた前処理・整形\n",
    "\n",
    "### 📌 ノイズ除去や整形の例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa65001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 改行・空白の正規化\n",
    "text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "# 特定の記号を除去\n",
    "text = re.sub(r\"[【】『』◇◆■□●○▲▼☆★▶︎→→]\", \"\", text)\n",
    "\n",
    "# セクション見出しで分割（例：\"第1章\"）\n",
    "chunks = re.split(r\"第\\d+章\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fa109d",
   "metadata": {},
   "source": [
    "## ✅ ステップ ④：ファイルごとのチャンク化\n",
    "\n",
    "### 📌 PDF の場合（PyMuPDF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "doc = fitz.open(\"document.pdf\")\n",
    "text = \"\\n\".join([page.get_text() for page in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a1efa",
   "metadata": {},
   "source": [
    "### 📌 Markdown/HTML/Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.md\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2481ee",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑤：チャンクにメタデータを付加\n",
    "\n",
    "### 📌 ファイル名・ページ番号などを保持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_records = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_records.append({\n",
    "        \"content\": chunk,\n",
    "        \"source\": \"document.pdf\",\n",
    "        \"page\": i + 1\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8dd50",
   "metadata": {},
   "source": [
    "## ✅ ステップ ⑥：ベクトルデータベース(Weaviate) への登録\n",
    "\n",
    "### 📌 1.コレクションを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07725124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wvcc\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Weaviateにローカル接続\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# スキーマが未作成の場合は事前に作成する\n",
    "if not client.collections.exists(\"DocumentChunk\"):\n",
    "    collection = client.collections.create(\n",
    "        name=\"DocumentChunk\",\n",
    "        description=\"チャンクしたデータを保存する演習用コレクション\",\n",
    "        properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"source\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"page\", data_type=wvcc.DataType.INT)\n",
    "        ],\n",
    "        vectorizer_config=wvcc.Configure.Vectorizer.text2vec_azure_openai(\n",
    "            base_url=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            deployment_id=os.environ.get(\n",
    "                \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_MODEL\"),\n",
    "            resource_name=os.environ.get(\"AZURE_OPENAI_RESOURCE_NAME\")\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    print(f\"すでにコレクションが存在します: DocumentChunk\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f82b20",
   "metadata": {},
   "source": [
    "### 📌 2.チャンクを登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de44a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Weaviateにローカル接続\n",
    "client = weaviate.connect_to_local()\n",
    "doc_collection = client.collections.get(\"DocumentChunk\")\n",
    "\n",
    "# チャンクを登録\n",
    "for i, record in enumerate(chunk_records):\n",
    "    # サンプルドキュメントを登録\n",
    "    # recordにはDocumentChunkで定義された書式のデータ\n",
    "    # 例) {\"content\": \"XXXX\", \"source\":\"YYYY\", \"page\": 1}\n",
    "    uuid = doc_collection.data.insert(properties=record)\n",
    "    print(f\"Inserted object no:{i}、UUID: {uuid}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d7b6d",
   "metadata": {},
   "source": [
    "## ✅ 実践課題\n",
    "\n",
    "### 📁 課題 1：任意のテキストファイルを 300 文字ごとに分割し、チャンク一覧を出力\n",
    "\n",
    "### 📁 課題 2：Markdown 文書から文単位チャンクを抽出し、章ごとのチャンク一覧を作成\n",
    "\n",
    "### 📁 課題 3：チャンクごとにメタデータ（セクション名・出典など）を保持し、CSV または JSON に出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be5dbb",
   "metadata": {},
   "source": [
    "### 🚀 最終ゴール\n",
    "\n",
    "- LLM やベクトル DB と組み合わせる前提として、**扱いやすく検索に適したテキストチャンクを自動生成**できるようになること。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
